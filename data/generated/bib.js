const generatedBibEntries = {
    "10094000": {
        "abstract": "Current supervised methods for 3D shape representation learning have achieved satisfying performance, yet require extensive human-labeled datasets. Unsupervised learning-based methods provide a viable solution by learning shape representations without using ground truth labels. In this study, we develop a contrastive learning framework for unsupervised representation learning of 3D shapes. Specifically, in order to encourage models to pay more attention to useful information during representation learning, we first introduce a new paradigm for critical points search based on the adversarial mechanism. We extract critical points with a larger impact on the global feature by attacking a pre-trained auto-encoder model, and apply data augmentations on these points to generate adversarial examples. Taking a pair of adversarial examples as inputs, we obtain their intermediate embeddings and global representations of corresponding inputs, which are then transformed into latent spaces by two predictor heads. Finally, we train the proposed model by maximizing the agreements on these latent spaces via Normalized Temperature-scaled Cross Entropy (NT-Xent) loss and a newly designed Cross-layer Normalized Temperature-scaled Cross Entropy (Cross-NT-Xent) loss, where the latter is proposed in this article to enforce cross-layer feature similarities. The effectiveness, robustness, and transferability of learned representations are validated on three downstream tasks, including object classification, few-shot classification, and shape retrieval. Experiments on three benchmark datasets show that our learned representations achieve better or competitive performance than current state-of-the-art methods in these downstream tasks. Moreover, our model can easily be extended to 3D part segmentation and scene segmentation tasks.",
        "author": "Wen, Congcong and Li, Xiang and Huang, Hao and Liu, Yu-Shen and Fang, Yi",
        "doi": "10.1109/TMM.2023.3265177",
        "journal": "IEEE Transactions on Multimedia",
        "keywords": "type:3D Vision, Shape,Three-dimensional displays,Point cloud compression,Representation learning,Task analysis,Feature extraction,Neural networks,3D shape representation,contrastive learning,adversarial examples,few-shot learning,shape retrieval",
        "number": "",
        "pages": "679-692",
        "title": "3D Shape Contrastive Representation Learning With Adversarial Examples",
        "type": "ARTICLE",
        "volume": "27",
        "year": "2025"
    },
    "GursoyDogan2025Aiao": {
        "abstract": "Purpose This study aims to offer an overview of hospitality and tourism research on artificial intelligence (AI) and its impact on the industry. More specifically, this study examines hospitality and tourism AI research trends in hospitality and tourism customer service experience creation and delivery, service failure and recovery, human resources and organizational behavior. Based on the review, this study identifies the challenges and opportunities and provides directions for future studies. Design/methodology/approach A narrative synthesis approach was used to review the hospitality and tourism research on AI and its impact on various aspects of the industry. Findings AI and AI applications in customer service experience creation and delivery and its possible effects on employees and organizations are viewed as a double-edged sword. Although the use of AI and AI applications offers various benefits, there are also serious concerns over the ethical use of AI, the replacement of human employees by AI-powered devices, discomfort among customers and employees and trust toward AI. Originality/value The paper offers an updated holistic overview of AI and its implications in different facets of the hospitality and tourism industry. Challenges and opportunities are discussed to foster future discussions on the use of AI among scholars and industry professionals.",
        "address": "Bradford",
        "author": "Gursoy, Dogan and Cai, Ruiying",
        "copyright": "Emerald Publishing Limited",
        "doi": "10.1108/IJCHM-03-2024-0322",
        "issn": "0959-6119",
        "journal": "International journal of contemporary hospitality management",
        "keywords": "type:AI Architecture and Applications, Artificial intelligence , Cognitive ability , Computer vision , Customer services , Emotions , Employees , Hospitality industry , Memory , Perceptions , Robots , Technology Acceptance Model , Technology adoption , Tourism",
        "language": "eng",
        "number": "1",
        "pages": "1-17",
        "publisher": "Emerald Publishing Limited",
        "title": "Artificial intelligence: an overview of research trends and future directions",
        "type": "article",
        "volume": "37",
        "year": "2025"
    },
    "MUZAHID2024128436": {
        "abstract": "With the growing availability of extensive 3D datasets and the rapid progress in computational power, deep learning (DL) has emerged as a highly promising approach for learning from 3D data, addressing critical tasks like object detection, segmentation, and recognition. Despite the unique challenges in processing geometry data with deep neural networks, recent advancements in DL for 3D object recognition have shown remarkable success, with various methods proposed to tackle different issues. This paper aims to stimulate future research by providing a comprehensive review of recent progress in DL techniques for 3D object recognition, which are systematically categorized based on their learning behavior. We discuss the advantages, limitations, and application of each approach, highlighting their performance in 3D object classification on benchmark datasets such as ModelNet, ScanObjectNN, and Sydney Urban Object. The survey offers insightful observations and inspires future research directions.",
        "author": "A.A.M. Muzahid and Hua Han and Yujin Zhang and Dawei Li and Yuhe Zhang and Junaid Jamshid and Ferdous Sohel",
        "doi": "https://doi.org/10.1016/j.neucom.2024.128436",
        "issn": "0925-2312",
        "journal": "Neurocomputing",
        "keywords": "type:3D Vision, 3D shape analysis, 3D object classification, Point cloud classification, Volumetric CNN",
        "number": "",
        "pages": "128436",
        "title": "Deep learning for 3D object recognition: A survey",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S0925231224012074",
        "volume": "608",
        "year": "2024"
    },
    "Xue2024PhyT2VLI": {
        "abstract": "Text-to-video (T2V) generation has been recently enabled by transformer-based diffusion models, but current T2V models lack capabilities in adhering to the real-world common knowledge and physical rules, due to their limited understanding of physical realism and deficiency in temporal modeling. Existing solutions are either data-driven or require extra model inputs, but cannot be generalizable to out-of-distribution domains. In this paper, we present PhyT2V, a new data-independent T2V technique that expands the current T2V model's capability of video generation to out-of-distribution domains, by enabling chain-of-thought and step-back reasoning in T2V prompting. Our experiments show that PhyT2V improves existing T2V models' adherence to real-world physical rules by 2.3x, and achieves 35% improvement compared to T2V prompt enhancers. The source codes are available at: https://github.com/pittisl/PhyT2V.",
        "author": "Qiyao Xue and Xiangyu Yin and Boyuan Yang and Wei Gao",
        "doi": "10.48550/arXiv.2412.00596",
        "journal": "arXiv preprint arXiv:2412.00596",
        "keywords": "type:Physical Perception AI, Text-to-video (T2V) generation, transformer-based diffusion models, physical realism, temporal modeling, real-world common knowledge, physical rules, out-of-distribution domains, PhyT2V, data-independent, chain-of-thought, step-back reasoning, T2V prompting, model adherence, prompt enhancers",
        "title": "PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation",
        "type": "article",
        "url": "https://api.semanticscholar.org/CorpusID:274436627",
        "volume": "abs/2412.00596",
        "year": "2024"
    },
    "budathoki2025adversarialrobustnessanalysisvisionlanguage": {
        "abstract": "Adversarial attacks have been fairly explored for computer vision and vision-language models. However, the avenue of adversarial attack for the vision language segmentation models (VLSMs) is still under-explored, especially for medical image analysis. Thus, we have investigated the robustness of VLSMs against adversarial attacks for 2D medical images with different modalities with radiology, photography, and endoscopy. The main idea of this project was to assess the robustness of the fine-tuned VLSMs specially in the medical domain setting to address the high risk scenario. First, we have fine-tuned pre-trained VLSMs for medical image segmentation with adapters. Then, we have employed adversarial attacks -- projected gradient descent (PGD) and fast gradient sign method (FGSM) -- on that fine-tuned model to determine its robustness against adversaries. We have reported models' performance decline to analyze the adversaries' impact. The results exhibit significant drops in the DSC and IoU scores after the introduction of these adversaries. Furthermore, we also explored universal perturbation but were not able to find for the medical images.",
        "archiveprefix": "arXiv",
        "author": "Anjila Budathoki and Manish Dhakal",
        "doi": "10.48550/arXiv.2505.02971",
        "eprint": "2505.02971",
        "journal": "arXiv preprint arXiv:2505.02971",
        "keywords": "type:AI Architecture and Applications,Adversarial attacks, Vision language segmentation models, Medical image analysis, Robustness, Fine-tuned VLSMs, 2D medical images, Radiology, Photography, Endoscopy, Projected gradient descent, Fast gradient sign method, Model performance, DSC scores, IoU scores, Universal perturbation",
        "primaryclass": "cs.CV",
        "title": "Adversarial Robustness Analysis of Vision-Language Models in Medical Image Segmentation",
        "type": "article",
        "url": "https://arxiv.org/abs/2505.02971",
        "year": "2025"
    },
    "jo2025artificialbehaviorintelligencetechnology": {
        "abstract": "Understanding and predicting human behavior has emerged as a core capability in various AI application domains such as autonomous driving, smart healthcare, surveillance systems, and social robotics. This paper defines the technical framework of Artificial Behavior Intelligence (ABI), which comprehensively analyzes and interprets human posture, facial expressions, emotions, behavioral sequences, and contextual cues. It details the essential components of ABI, including pose estimation, face and emotion recognition, sequential behavior analysis, and context-aware modeling. Furthermore, we highlight the transformative potential of recent advances in large-scale pretrained models, such as large language models (LLMs), vision foundation models, and multimodal integration models, in significantly improving the accuracy and interpretability of behavior recognition. Our research team has a strong interest in the ABI domain and is actively conducting research, particularly focusing on the development of intelligent lightweight models capable of efficiently inferring complex human behaviors. This paper identifies several technical challenges that must be addressed to deploy ABI in real-world applications including learning behavioral intelligence from limited data, quantifying uncertainty in complex behavior prediction, and optimizing model structures for low-power, real-time inference. To tackle these challenges, our team is exploring various optimization strategies including lightweight transformers, graph-based recognition architectures, energy-aware loss functions, and multimodal knowledge distillation, while validating their applicability in real-time environments.",
        "archiveprefix": "arXiv",
        "author": "Kanghyun Jo and Jehwan Choi and Kwanho Kim and Seongmin Kim and Duy-Linh Nguyen and Xuan-Thuy Vo and Adri Priadana and Tien-Dat Tran",
        "doi": "10.48550/arXiv.2505.03315",
        "eprint": "2505.03315",
        "journal": "arXiv preprint arXiv:2505.03315",
        "keywords": "type:AI Architecture and Applications, Artificial Behavior Intelligence, Human behavior prediction, Pose estimation, Emotion recognition, Behavior analysis, Context-aware modeling, Large language models, Multimodal integration, Lightweight models, Real-time inference, Model optimization, Lightweight transformers, Graph-based architectures, Knowledge distillation",
        "primaryclass": "cs.AI",
        "title": "Artificial Behavior Intelligence: Technology, Challenges, and Future Directions",
        "type": "article",
        "url": "https://arxiv.org/abs/2505.03315",
        "year": "2025"
    },
    "lin2025exploringevolutionphysicscognition": {
        "abstract": "Recent advancements in video generation have witnessed significant progress, especially with the rapid advancement of diffusion models. Despite this, their deficiencies in physical cognition have gradually received widespread attention - generated content often violates the fundamental laws of physics, falling into the dilemma of ''visual realism but physical absurdity\". Researchers began to increasingly recognize the importance of physical fidelity in video generation and attempted to integrate heuristic physical cognition such as motion representations and physical knowledge into generative systems to simulate real-world dynamic scenarios. Considering the lack of a systematic overview in this field, this survey aims to provide a comprehensive summary of architecture designs and their applications to fill this gap. Specifically, we discuss and organize the evolutionary process of physical cognition in video generation from a cognitive science perspective, while proposing a three-tier taxonomy: 1) basic schema perception for generation, 2) passive cognition of physical knowledge for generation, and 3) active cognition for world simulation, encompassing state-of-the-art methods, classical paradigms, and benchmarks. Subsequently, we emphasize the inherent key challenges in this domain and delineate potential pathways for future research, contributing to advancing the frontiers of discussion in both academia and industry. Through structured review and interdisciplinary analysis, this survey aims to provide directional guidance for developing interpretable, controllable, and physically consistent video generation paradigms, thereby propelling generative models from the stage of ''visual mimicry'' towards a new phase of ''human-like physical comprehension''.",
        "archiveprefix": "arXiv",
        "author": "Minghui Lin and Xiang Wang and Yishan Wang and Shu Wang and Fengqi Dai and Pengxiang Ding and Cunxiang Wang and Zhengrong Zuo and Nong Sang and Siteng Huang and Donglin Wang",
        "doi": "10.48550/arXiv.2503.21765",
        "eprint": "2503.21765",
        "journal": "arXiv preprint arXiv:2503.21765",
        "keywords": "type:Physical Perception AI, Video generation, Diffusion models, Physical cognition, Physical fidelity, Real-world dynamics, Cognitive science, Three-tier taxonomy, World simulation, Interpretable generation, Physically consistent generation",
        "primaryclass": "cs.CV",
        "title": "Exploring the Evolution of Physics Cognition in Video Generation: A Survey",
        "type": "article",
        "url": "https://arxiv.org/abs/2503.21765",
        "year": "2025"
    },
    "liu2025generativephysicalaivision": {
        "abstract": "Generative Artificial Intelligence (AI) has rapidly advanced the field of computer vision by enabling machines to create and interpret visual data with unprecedented sophistication. This transformation builds upon a foundation of generative models to produce realistic images, videos, and 3D/4D content. Conventional generative models primarily focus on visual fidelity while often neglecting the physical plausibility of the generated content. This gap limits their effectiveness in applications that require adherence to real-world physical laws, such as robotics, autonomous systems, and scientific simulations. As generative models evolve to increasingly integrate physical realism and dynamic simulation, their potential to function as \"world simulators\" expands. Therefore, the field of physics-aware generation in computer vision is rapidly growing, calling for a comprehensive survey to provide a structured analysis of current efforts. To serve this purpose, the survey presents a systematic review, categorizing methods based on how they incorporate physical knowledge, either through explicit simulation or implicit learning. It also analyzes key paradigms, discusses evaluation protocols, and identifies future research directions. By offering a comprehensive overview, this survey aims to help future developments in physically grounded generation for computer vision. The reviewed papers are summarized at https://tinyurl.com/Physics-Aware-Generation.",
        "archiveprefix": "arXiv",
        "author": "Daochang Liu and Junyu Zhang and Anh-Dung Dinh and Eunbyung Park and Shichao Zhang and Ajmal Mian and Mubarak Shah and Chang Xu",
        "doi": "10.48550/arXiv.2501.10928",
        "eprint": "2501.10928",
        "journal": "arXiv preprint arXiv:2501.10928",
        "keywords": "type:Physical Perception AI, Generative Artificial Intelligence, computer vision, generative models, visual data, visual fidelity, physical plausibility, real-world physical laws, robotics, autonomous systems, scientific simulations, world simulators, physics-aware generation, dynamic simulation, systematic review, physical knowledge, explicit simulation, implicit learning, evaluation protocols, future research directions, physically grounded generation",
        "primaryclass": "cs.CV",
        "title": "Generative Physical AI in Vision: A Survey",
        "type": "article",
        "url": "https://arxiv.org/abs/2501.10928",
        "year": "2025"
    },
    "wang2025modularmachinelearningindispensable": {
        "abstract": "Large language models (LLMs) have dramatically advanced machine learning research including natural language processing, computer vision, data mining, etc., yet they still exhibit critical limitations in reasoning, factual consistency, and interpretability. In this paper, we introduce a novel learning paradigm -- Modular Machine Learning (MML) -- as an essential approach toward new-generation LLMs. MML decomposes the complex structure of LLMs into three interdependent components: modular representation, modular model, and modular reasoning, aiming to enhance LLMs' capability of counterfactual reasoning, mitigating hallucinations, as well as promoting fairness, safety, and transparency. Specifically, the proposed MML paradigm can: i) clarify the internal working mechanism of LLMs through the disentanglement of semantic components; ii) allow for flexible and task-adaptive model design; iii) enable interpretable and logic-driven decision-making process. We present a feasible implementation of MML-based LLMs via leveraging advanced techniques such as disentangled representation learning, neural architecture search and neuro-symbolic learning. We critically identify key challenges, such as the integration of continuous neural and discrete symbolic processes, joint optimization, and computational scalability, present promising future research directions that deserve further exploration. Ultimately, the integration of the MML paradigm with LLMs has the potential to bridge the gap between statistical (deep) learning and formal (logical) reasoning, thereby paving the way for robust, adaptable, and trustworthy AI systems across a wide range of real-world applications.",
        "archiveprefix": "arXiv",
        "author": "Xin Wang and Haoyang Li and Zeyang Zhang and Haibo Chen and Wenwu Zhu",
        "doi": "10.48550/arXiv.2504.20020",
        "eprint": "2504.20020",
        "journal": "arXiv preprint arXiv:2504.20020",
        "keywords": "type:AI Architecture and Applications, Large language models, Modular Machine Learning, Modular representation, Modular model, Modular reasoning, Counterfactual reasoning, Hallucination mitigation, Interpretability, Fairness, Transparency, Disentangled representation learning, Neural architecture search, Neuro-symbolic learning, Computational scalability, Logical reasoning",
        "primaryclass": "cs.LG",
        "title": "Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models",
        "type": "article",
        "url": "https://arxiv.org/abs/2504.20020",
        "year": "2025"
    },
    "zhao2025diffusionsfmpredictingstructuremotion": {
        "abstract": "Current Structure-from-Motion (SfM) methods typically follow a two-stage pipeline, combining learned or geometric pairwise reasoning with a subsequent global optimization step. In contrast, we propose a data-driven multi-view reasoning approach that directly infers 3D scene geometry and camera poses from multi-view images. Our framework, DiffusionSfM, parameterizes scene geometry and cameras as pixel-wise ray origins and endpoints in a global frame and employs a transformer-based denoising diffusion model to predict them from multi-view inputs. To address practical challenges in training diffusion models with missing data and unbounded scene coordinates, we introduce specialized mechanisms that ensure robust learning. We empirically validate DiffusionSfM on both synthetic and real datasets, demonstrating that it outperforms classical and learning-based approaches while naturally modeling uncertainty.",
        "archiveprefix": "arXiv",
        "author": "Qitao Zhao and Amy Lin and Jeff Tan and Jason Y. Zhang and Deva Ramanan and Shubham Tulsiani",
        "doi": "10.48550/arXiv.2505.05473",
        "eprint": "2505.05473",
        "journal": "arXiv preprint arXiv:2505.05473",
        "keywords": "type:3D Vision, Structure-from-Motion, Multi-view reasoning, 3D scene geometry, Camera poses, DiffusionSfM, Transformer-based diffusion, Pixel-wise parameterization, Robust learning, Uncertainty modeling, Synthetic datasets, Real datasets",
        "primaryclass": "cs.CV",
        "title": "DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion",
        "type": "article",
        "url": "https://arxiv.org/abs/2505.05473",
        "year": "2025"
    }
};